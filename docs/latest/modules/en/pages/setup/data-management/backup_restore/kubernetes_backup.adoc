= Kubernetes backup (Legacy)
:revdate: 2025-07-10
:page-revdate: {revdate}
:description: SUSE Observability Self-hosted

[WARNING]
====
*This page describes the legacy backup approach using scripts. For {stackstate-product-name} v2.7.0 or newer, use the new xref:/setup/data-management/backup_restore/backup_enable.adoc[backup CLI] instead.*

**Breaking changes in v2.7.0:**

* `backup.enabled` is replaced with `global.backup.enabled` - Helm deployments will fail if `backup.enabled` is set to `true`
* All backups are controlled with a single value `global.backup.enabled` instead of per-datastore enablement
* `*.restore.enabled` values have been removed
====

== Overview

{stackstate-product-name} has a built-in backup and restore mechanism that can be configured to store backups to the local clusters, to AWS S3 or to Azure Blob Storage.

=== Backup scope

The following data can be automatically backed up:

* *Configuration and topology data* stored in StackGraph
* *Metrics* stored in SUSE Observability's Victoria Metrics instance(s)
* *Telemetry data* stored in SUSE Observability's Elasticsearch instance
* *OpenTelemetry data* stored in SUSE Observability's ClickHouse instance

The following data will *not* be backed up:

* In transit topology and telemetry updates stored in Kafka - these only have temporary value and would be of no use when a backup is restored
* Master node negotiations state stored in ZooKeeper - this runtime state would be incorrect when restored and will be automatically determined at runtime
* Kubernetes configuration state and raw persistent volume state - this state can be rebuilt by re-installing SUSE Observability and restoring the backups.
* Kubernetes logs - these are ephemeral.

=== Storage options

Backups are sent to an instance of https://min.io/[MinIO (min.io)], which is automatically started by the `suse-observability` Helm chart when automatic backups are enabled. MinIO is an object storage system with the same API as AWS S3. It can store its data locally or act as a gateway to https://docs.min.io/docs/minio-gateway-for-s3.html[AWS S3 (min.io)], https://docs.min.io/docs/minio-gateway-for-azure.html[Azure BLob Storage (min.io)] and other systems.

The built-in MinIO instance can be configured to store the backups in three locations:

* xref:/setup/data-management/backup_restore/kubernetes_backup.adoc[AWS S3]
* xref:/setup/data-management/backup_restore/kubernetes_backup.adoc[Azure Blob Storage]
* xref:/setup/data-management/backup_restore/kubernetes_backup.adoc[Kubernetes storage]

== Enable backups

=== AWS S3

[CAUTION]
====

*Encryption*

Amazon S3-managed keys (SSE-S3) should be used when encrypting S3 buckets that store the backups.

⚠️ Encryption with AWS KMS keys stored in AWS Key Management Service (SSE-KMS) isn't supported. This will result in errors such as this one in the Elasticsearch logs:

`Caused by: org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper: sdk_client_exception: Unable to verify integrity of data upload. Client calculated content hash (contentMD5: ZX4D/ZDUzZWRhNDUyZTI1MTc= in base 64) didn't match hash (etag: c75faa31280154027542f6530c9e543e in hex) calculated by Amazon S3. You may need to delete the data stored in Amazon S3. (metadata.contentMD5: null, md5DigestStream: com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream@5481a656, bucketName: suse-observability-elasticsearch-backup, key: tests-UG34QIV9s32tTzQWdPsZL/master.dat)",`
====


To enable scheduled backups to AWS S3 buckets, add the following YAML fragment to the Helm `values.yaml` file used to install SUSE Observability:

[,yaml]
----
global:
  backup:
    enabled: true
backup:
  stackGraph:
    bucketName: AWS_STACKGRAPH_BUCKET
  elasticsearch:
    bucketName: AWS_ELASTICSEARCH_BUCKET
  configuration:
    bucketName: AWS_CONFIGURATION_BUCKET
victoria-metrics-0:
  backup:
    bucketName: AWS_VICTORIA_METRICS_BUCKET
victoria-metrics-1:
  backup:
    bucketName: AWS_VICTORIA_METRICS_BUCKET
clickhouse:
  backup:
    bucketName: AWS_CLICKHOUSE_BUCKET
minio:
  accessKey: YOUR_ACCESS_KEY
  secretKey: YOUR_SECRET_KEY
  s3gateway:
    enabled: true
    accessKey: AWS_ACCESS_KEY
    secretKey: AWS_SECRET_KEY
----

Replace the following values:

* `YOUR_ACCESS_KEY` and `YOUR_SECRET_KEY` are the credentials that will be used to secure the MinIO system. These credentials are set on the MinIO system and used by the automatic backup jobs and the restore jobs. They're also required if you want to manually access the MinIO system.
 ** YOUR_ACCESS_KEY should contain 5 to 20 alphanumerical characters.
 ** YOUR_SECRET_KEY should contain 8 to 40 alphanumerical characters.
* `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` are the AWS credentials for the IAM user that has access to the S3 buckets where the backups will be stored. See below for the permission policy that needs to be attached to that user.
* `AWS_STACKGRAPH_BUCKET`, `AWS_CONFIGURATION_BUCKET`, `AWS_ELASTICSEARCH_BUCKET`, `AWS_VICTORIA_METRICS_BUCKET` and `AWS_CLICKHOUSE_BUCKET` are the names of the S3 buckets where the backups should be stored. Note: The names of AWS S3 buckets are global across the whole of AWS, therefore the S3 buckets with the default name (`sts-elasticsearch-backup`, `sts-configuration-backup`, `sts-stackgraph-backup`, `sts-victoria-metrics-backup` and `sts-clickhouse-backup` ) will probably not be available.

The IAM user identified by `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` must be configured with the following permission policy to access the S3 buckets:

[,javascript]
----
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AllowListMinioBackupBuckets",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation"
            ],
            "Resource": [
                "arn:aws:s3:::AWS_STACKGRAPH_BUCKET",
                "arn:aws:s3:::AWS_ELASTICSEARCH_BUCKET",
                "arn:aws:s3:::AWS_VICTORIA_METRICS_BUCKET",
                "arn:aws:s3:::AWS_CLICKHOUSE_BUCKET",
                "arn:aws:s3:::AWS_CONFIGURATION_BUCKET"
            ]
        },
        {
            "Sid": "AllowWriteMinioBackupBuckets",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::AWS_STACKGRAPH_BUCKET/*",
                "arn:aws:s3:::AWS_ELASTICSEARCH_BUCKET/*",
                "arn:aws:s3:::AWS_VICTORIA_METRICS_BUCKET/*",
                "arn:aws:s3:::AWS_CLICKHOUSE_BUCKET/*",
                "arn:aws:s3:::AWS_CONFIGURATION_BUCKET"
            ]
        }
    ]
}
----

=== Azure Blob Storage

To enable backups to an Azure Blob Storage account, add the following YAML fragment to the Helm `values.yaml` file used to install SUSE Observability:

[,yaml]
----
global:
  backup:
    enabled: true
minio:
  accessKey: AZURE_STORAGE_ACCOUNT_NAME
  secretKey: AZURE_STORAGE_ACCOUNT_KEY
  azuregateway:
    enabled: true
----

Replace the following values:

* `AZURE_STORAGE_ACCOUNT_NAME` - the https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal[Azure storage account name (learn.microsoft.com)]
* `AZURE_STORAGE_ACCOUNT_KEY` - the https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal[Azure storage account key (learn.microsoft.com)] where the backups should be stored.

The StackGraph, Elasticsearch and Victoria Metrics backups are stored in BLOB containers called `sts-stackgraph-backup`, `sts-configuration-backup`, `sts-elasticsearch-backup`, `sts-victoria-metrics-backup`, `sts-clickhouse-backup` respectively. These names can be changed by setting the Helm values `backup.stackGraph.bucketName`, `backup.elasticsearch.bucketName`, `victoria-metrics-0.backup.bucketName`, `victoria-metrics-1.backup.bucketName` and `clickhouse.backup.bucketName` respectively.

=== Kubernetes storage

[CAUTION]
====
If MinIO is configured to store its data in Kubernetes storage, a PersistentVolumeClaim (PVC) is used to request storage from the Kubernetes cluster. The kind of storage allocated depends on the configuration of the cluster.

It's advised to use AWS S3 for clusters running on Amazon AWS and Azure Blob Storage for clusters running on Azure for the following reasons:

. Kubernetes clusters running in a cloud provider usually map PVCs to block storage, such as Elastic Block Storage for AWS or Azure Block Storage. Block storage is expensive, especially for large data volumes.
. Persistent Volumes are destroyed when the cluster that created them is destroyed. That means an (accidental) deletion of your cluster will also destroy all backups stored in Persistent Volumes.
. Persistent Volumes can't be accessed from another cluster. That means that it isn't possible to restore SUSE Observability from a backup taken on another cluster.
====


To enable backups to cluster-local storage, enable MinIO by adding the following YAML fragment to the Helm `values.yaml` file used to install SUSE Observability:

[,yaml]
----
global:
  backup:
    enabled: true
minio:
  accessKey: YOUR_ACCESS_KEY
  secretKey: YOUR_SECRET_KEY
  persistence:
    enabled: true
----

Replace the following values:

* `YOUR_ACCESS_KEY` and `YOUR_SECRET_KEY` - the credentials that will be used to secure the MinIO system. The automatic backup jobs and the restore jobs will use them. They're also required to manually access the MinIO storage. `YOUR_ACCESS_KEY` should contain 5 to 20 alphanumerical characters and `YOUR_SECRET_KEY` should contain 8 to 40 alphanumerical characters.

== Configuration and topology data (StackGraph)

Configuration and topology data (StackGraph) backups are full backups, stored in a single file with the extension `.graph`. Each file contains a full backup and can be moved, copied or deleted as required.

=== Backup schedule

By default, the StackGraph backups are created daily at 03:00 AM server time.

The backup schedule can be configured using the Helm value `backup.stackGraph.scheduled.schedule`, specified in https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#_cron_schedule_syntax[Kubernetes cron schedule syntax (kubernetes.io)].

=== Backup retention

By default, the StackGraph backups are kept for 30 days. As StackGraph backups are full backups, this can require a lot of storage.

The backup retention delta can be configured using the Helm value `backup.stackGraph.scheduled.backupRetentionTimeDelta`, specified in the format of GNU date `--date` argument. For example, the default is `30 days ago`. See https://www.gnu.org/software/coreutils/manual/html_node/Relative-items-in-date-strings.html[Relative items in date strings] for more examples.

=== Disable scheduled backups

To disable scheduled StackGraph backups, set the backup schedule to a date far in the past using the Helm value `backup.stackGraph.scheduled.schedule`:

[,yaml]
----
backup:
  stackGraph:
    scheduled:
      schedule: '0 0 1 1 1970'  # January 1, 1970 (epoch start)
----

== Metrics (Victoria Metrics)

[WARNING]
====
Victoria Metrics use incremental backups without versioning of a bucket, it means that the new backup *replaces completely the previous one*.

In case you run into one of the following situations:

* mount an empty volume to `/storage` directory of Victoria Metrics instances
* delete the `/storage` directory or files inside from Victoria Metrics instances

The next (empty) backup created will be labeled with a new version and the previous one, before the volume was emptied, will be preserved.
Both backups will be from that moment on <<_list_victoria_metrics_backups,listed as available for restore>>.
====


Metrics (Victoria Metrics) use instant snapshots to store data in incremental backups. Many instances of Victoria Metrics can store backups to the same bucket, each of them will be stored in separated directory. All files located in the directory should be treated as a single whole and can only be moved, copied or deleted as a whole.

[NOTE]
====
High Available deployments should be deployed with two instances of Victoria Metrics. Backups are enabled/configured independently for each of them.

The following code snippets/commands are provided for the first instance of Victoria Metric `victoria-metrics-0`. To backup/configure the second instance you should use `victoria-metrics-1`
====


=== Backup schedule

By default, the Victoria Metrics backups are created every 1h:

* `victoria-metrics-0` - 25 minutes past the hour
* `victoria-metrics-1` - 35 minutes past the hour

The backup schedule can be configured using the Helm value `victoria-metrics-0.backup.scheduled.schedule` according https://github.com/aptible/supercronic/tree/master/cronexpr[cronexpr format]

=== Disable scheduled backups

To disable scheduled Victoria Metrics backups, set the backup schedule for both instances to a date far in the past:

[,yaml]
----
victoria-metrics-0:
  backup:
    scheduled:
      schedule: '0 0 1 1 1970'  # January 1, 1970 (epoch start)
victoria-metrics-1:
  backup:
    scheduled:
      schedule: '0 0 1 1 1970'  # January 1, 1970 (epoch start)
----

== OpenTelemetry (ClickHouse)

ClickHouse uses both incremental and full backups. By default, full backups are executed daily at 00:45 am, and incremental backups are performed every hour. Each backup creates a new directory and old backups (directories) are deleted automatically. All files located in a backup directory are treated as a singular group and can only be moved, copied or deleted as a group. It's recommended to use the `clickhouse-backup` tool available on the `suse-observability-clickhouse-shard0-0` Pod to manage backups.

=== Backup schedule

By default, the ClickHouse backups are created:

* Full Backup - at 00:45 every day
* Incremental Backup - 45 minutes past the hour (from 3 am to 12 am)

[CAUTION]
====
Backups struggle with parallel execution. If a second backup starts before the first one completes, it will disrupt the first backup. Therefore, it's crucial to avoid parallel execution. For instance, the first incremental backup should be executed three hours after the full one.
====


The backup schedule can be configured using the Helm value `clickhouse.backup.scheduled.full_schedule` and `clickhouse.backup.scheduled.incremental_schedule` according https://github.com/aptible/supercronic/tree/master/cronexpr[cronexpr format]

=== Backup retention

By default, the tooling keeps last 308 backups (full and incremental) what is equal to ~14 days.

The backup retention can be configured using the Helm value `clickhouse.backup.config.keep_remote`.

=== Disable scheduled backups

To disable scheduled ClickHouse backups, set both full and incremental backup schedules to a date far in the past:

[,yaml]
----
clickhouse:
  backup:
    scheduled:
      full_schedule: '0 0 1 1 1970'         # January 1, 1970 (epoch start)
      incremental_schedule: '0 0 1 1 1970'  # January 1, 1970 (epoch start)
----

== Telemetry data (Elasticsearch)

The telemetry data (Elasticsearch) snapshots are incremental and stored in files with the extension `.dat`. The files in the Elasticsearch backup storage location should be treated as a single whole and can only be moved, copied or deleted as a whole.

The configuration snippets provided in the section xref:/setup/data-management/backup_restore/kubernetes_backup.adoc#_enable_backups[enable backups] will enable daily Elasticsearch snapshots.

=== Disable scheduled snapshots

To disable scheduled Elasticsearch snapshots, set the snapshot schedule to a date far in the past using the Helm value `backup.elasticsearch.scheduled.schedule`:

[,yaml]
----
backup:
  elasticsearch:
    scheduled:
      schedule: '0 0 1 1 1970'  # January 1, 1970 (epoch start)
----

=== Snapshot schedule

By default, Elasticsearch snapshots are created daily at 03:00 AM server time.

The backup schedule can be configured using the Helm value `backup.elasticsearch.scheduled.schedule`, specified in https://www.elastic.co/guide/en/elasticsearch/reference/7.6/cron-expressions.html[Elasticsearch cron schedule syntax (elastic.co)].

=== Snapshot retention

By default, Elasticsearch snapshots are kept for 30 days, with a minimum of 5 snapshots and a maximum of 30 snapshots.

The retention time and number of snapshots kept can be configured using the following Helm values:

* `backup.elasticsearch.scheduled.snapshotRetentionExpireAfter`, specified in https://www.elastic.co/guide/en/elasticsearch/reference/7.6/common-options.html#_time_units[Elasticsearch time units (elastic.co)].
* `backup.elasticsearch.scheduled.snapshotRetentionMinCount`
* `backup.elasticsearch.scheduled.snapshotRetentionMaxCount`

[NOTE]
====
By default, the retention task itself https://www.elastic.co/guide/en/elasticsearch/reference/7.6/slm-settings.html#_slm_retention_schedule[runs daily at 1:30 AM UTC (elastic.co)]. If you set snapshots to expire faster than within a day, for example for testing purposes, you will need to change the schedule for the retention task.
====


=== Snapshot indices

By default, a snapshot is created for Elasticsearch indices with names that start with `sts`.

The indices for which a snapshot is created can be configured using the Helm value `backup.elasticsearch.scheduled.indices`, specified in https://www.w3schools.com/js/js_json_arrays.asp[JSON array format (w3schools.com)].

== Restore backups and snapshots

Scripts to list and restore backups and snapshots can be downloaded from the https://github.com/StackVista/helm-charts/releases/latest[latest release for the SUSE Observability Helm chart]. Download and extract the `backup-scripts-<version>.tar.gz` to get started.

[NOTE]
====
*Before you use the scripts, ensure that:*

* The `kubectl` binary is installed and configured to connect to:
 .. The Kubernetes cluster where SUSE Observability has been installed.
 .. The namespace within that cluster where SUSE Observability has been installed.
* The Helm value `global.backup.enabled` is set to `true`.
====


=== List StackGraph backups

To list the StackGraph backups, execute the following command:

[,bash]
----
./restore/list-stackgraph-backups.sh
----

The output should look like this:

[,bash]
----
job.batch/stackgraph-list-backups-20210222t111942 created
Waiting for job to start...
=== Listing StackGraph backups in bucket "sts-stackgraph-backup"...
sts-backup-20210215-0300.graph
sts-backup-20210216-0300.graph
sts-backup-20210217-0300.graph
sts-backup-20210218-0300.graph
sts-backup-20210219-0300.graph
sts-backup-20210220-0300.graph
sts-backup-20210221-0300.graph
sts-backup-20210222-0300.graph
===
job.batch "stackgraph-list-backups-20210222t111942" deleted
----

The timestamp when the backup was taken is part of the backup name.

[NOTE]
====
Lines in the output that start with `Error from server (BadRequest):` are expected. They appear when the script is waiting for the pod to start.
====


=== Restore a StackGraph backup

[CAUTION]
====
*To avoid the unexpected loss of existing data, a backup can only be restored on a clean environment by default.*
If you are completely sure that any existing data can be overwritten, you can override this safety feature by using the command `-force`.
Only execute the restore command when you are sure that you want to restore the backup.
====


To restore a StackGraph backup on a clean environment, select a backup name and pass it as the first parameter in the following command:

[,bash]
----
./restore/restore-stackgraph-backup.sh sts-backup-20210216-0300.graph
----

To restore a StackGraph backup on an *environment with existing data*, select a backup name and pass it as the first parameter in the following command next to a second parameter `-force`:
[NOTE]
====
*Note that existing data will be overwritten when the backup is restored.*

Only do this if you are completely sure that any existing data can be overwritten.
====


[,bash]
----
./restore/restore-stackgraph-backup.sh sts-backup-20210216-0300.graph -force
----

The output should look like this:

[,bash]
----
job.batch/stackgraph-restore-20210222t112142 created
Waiting for job to start...
=== Downloading StackGraph backup "sts-backup-20210216-0300.graph" from bucket "sts-stackgraph-backup"...
download: s3://sts-stackgraph-backup/sts-backup-20210216-1252.graph to ../../tmp/sts-backup-20210216-0300.graph
=== Importing StackGraph data from "sts-backup-20210216-0300.graph"...
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.codehaus.groovy.vmplugin.v7.Java7$1 (file:/opt/docker/lib/org.codehaus.groovy.groovy-2.5.4.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class,int)
WARNING: Please consider reporting this to the maintainers of org.codehaus.groovy.vmplugin.v7.Java7$1
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
===
job.batch "stackgraph-restore-20210222t112142" deleted
----

In case you are running a restore command missing the `-force` flag on a non-empty database the output will contain an error like this:

[,bash]
----
ERROR com.stackvista.graph.migration.Restore - Restore isn't possible in a non empty.
----

[NOTE]
====
Lines that starts with `WARNING:` are expected. They're generated by Groovy running in JDK 11 and can be ignored.
====


=== List Victoria Metrics backups

To list the Victoria Metrics backups, execute the following command:

[,bash]
----
./restore/list-victoria-metrics-backups.sh
----

The output should look like this:

[,bash]
----
job.batch/victoria-metrics-list-backups-20231016t125557 created
Waiting for job to start...
Waiting for job to start...
=== Fetching timestamps of last completed incremental backups
victoria-metrics-0 victoria-metrics-0-20231128160000 "Wed, 29 Nov 2023 07:36:00 GMT"
victoria-metrics-0 victoria-metrics-0-20231129092200 "Wed, 29 Nov 2023 10:56:00 GMT"

===
job.batch "victoria-metrics-list-backups-20231016t125557" deleted
----

where you can see the Victoria metrics instance, the specific backup version and the last time a backup was completed.

=== Restore a Victoria Metrics backup

[CAUTION]
====
*Restore functionality always overrides data. You must be careful to avoid the unexpected loss of existing data.*
====


[CAUTION]
====
*Restore functionality requires to stop an instance of Victoria Metric while the process.*

All new metrics will be cached by `vmagent` while the restore process, please ensure the `vmagent` has enough memory to cache metrics.
====


To restore a Victoria Metrics backup, select an instance name and a backup version and pass them as parameters in the following command:

[,bash]
----
./restore/restore-victoria-metrics-backup.sh victoria-metrics-0 victoria-metrics-0-20231128160000
----

The output should look like this:

[,bash]
----
=== Scaling down the Victoria Metrics instance
statefulset.apps/suse-observability-victoria-metrics-0 scaled
=== Allowing pods to terminate
=== Starting restore job
job.batch/victoria-metrics-restore-backup-20231017t092607 created
=== Restore job started. Follow the logs with the following command:
kubectl logs --follow job/victoria-metrics-restore-backup-20231017t092607
=== After the job has completed clean up the job and scale up the Victoria Metrics instance pods again with the following commands:
kubectl delete job victoria-metrics-restore-backup-20231017t092607
kubectl scale statefulsets suse-observability-victoria-metrics-0 --replicas=1
----

Then follow logs to check the job status

----
2023-10-17T07:26:42.564Z	info	VictoriaMetrics/lib/backup/actions/restore.go:194	restored 53072307269 bytes from backup in 0.445 seconds; deleted 639118752 bytes; downloaded 1204539 bytes
2023-10-17T07:26:42.564Z	info	VictoriaMetrics/app/vmrestore/main.go:64	gracefully shutting down http server for metrics at ":8421"
2023-10-17T07:26:42.564Z	info	VictoriaMetrics/app/vmrestore/main.go:68	successfully shut down http server for metrics in 0.000 seconds
----

After completion (*ensure if the backup has been restored successfully*), it's needed to follow commands printed by the earlier command:

* delete the restore job
* scale up the Victoria Metrics instance

=== List ClickHouse backups

[NOTE]
====
The following script needs permission to execute the `kubectl exec` command.
====


To list ClickHouse backups, execute the following command:

[,bash]
----
./restore/list-clickhouse-backups.sh
----

The output should look like this:

[,bash]
----
full_2024-06-17T18-50-00          34.41KiB   17/06/2024 18:50:00   remote                                      tar, regular
incremental_2024-06-17T18-51-00   7.29KiB    17/06/2024 18:51:00   remote   +full_2024-06-17T18-50-00          tar, regular
incremental_2024-06-17T18-54-00   7.29KiB    17/06/2024 18:54:00   remote   +incremental_2024-06-17T18-51-00   tar, regular
incremental_2024-06-17T18-57-00   7.29KiB    17/06/2024 18:57:00   remote   +incremental_2024-06-17T18-54-00   tar, regular
full_2024-06-17T19-00-00          26.41KiB   17/06/2024 19:00:00   remote                                      tar, regular
incremental_2024-06-17T19-00-00   6.52KiB    17/06/2024 19:00:00   remote   +incremental_2024-06-17T18-57-00   tar, regular
incremental_2024-06-17T19-03-00   25.37KiB   17/06/2024 19:03:00   remote   +incremental_2024-06-17T19-00-00   tar, regular
incremental_2024-06-17T19-06-00   7.29KiB    17/06/2024 19:06:00   remote   +incremental_2024-06-17T19-03-00   tar, regular
----

where is printed:

* name, the name started with `full_` - it is a full backup, `incremental_` - it is an incremental backup
* size,
* creation date,
* `remote` - a backup is upload to a remote storage like S3
* parent backup - used by incremental backups
* format and compression

=== Restore a ClickHouse backup

[CAUTION]
====
*Restore functionality overwrites the data. All tables in the `otel` database are dropped and restored from the backup. Beware to avoid unexpected data loss.*
====


[NOTE]
====
The following script needs permission to execute the `kubectl exec` command.
====


[CAUTION]
====
*Restore functionality requires stopping all producers (like OpenTelemetry exporters). The script scales down the workloads before the backup, and scales up the workloads after the backup finishes.*
====


To restore a ClickHouse backup, select a backup version and pass them as a parameter in the following command:

[,bash]
----
./restore/restore-clickhouse-backup.sh incremental_2024-06-17T18-57-00
----

The output should look like this:

[,bash]
----
...
2024/06/17 19:14:19.509498  info download object_disks start backup=incremental_2024-06-17T19-06-00 operation=restore_data table=otel.otel_traces_trace_id_ts_mv
2024/06/17 19:14:19.509530  info download object_disks finish backup=incremental_2024-06-17T19-06-00 duration=0s operation=restore_data size=0B table=otel.otel_traces_trace_id_ts_mv
2024/06/17 19:14:19.509549  info done                      backup=incremental_2024-06-17T19-06-00 duration=0s operation=restore_data progress=12/12 table=otel.otel_traces_trace_id_ts_mv
2024/06/17 19:14:19.509574  info done                      backup=incremental_2024-06-17T19-06-00 duration=66ms operation=restore_data
2024/06/17 19:14:19.509591  info done                      backup=incremental_2024-06-17T19-06-00 duration=167ms operation=restore version=2.5.13
2024/06/17 19:14:19.509684  info clickhouse connection closed logger=clickhouse
Data restored
----

=== List Elasticsearch snapshots

To list the Elasticsearch snapshots, execute the following command:

[,bash]
----
./restore/list-elasticsearch-snapshots.sh
----

The output should look like this:

[,bash]
----
job.batch/elasticsearch-list-snapshots-20210224t133115 created
Waiting for job to start...
Waiting for job to start...
=== Listing Elasticsearch snapshots in snapshot repository "sts-backup" in bucket "sts-elasticsearch-backup"...
sts-backup-20210219-0300-mref7yrvrswxa02aqq213w
sts-backup-20210220-0300-yrn6qexkrdgh3pummsrj7e
sts-backup-20210221-0300-p481sih8s5jhre9zy4yw2o
sts-backup-20210222-0300-611kxendsvh4hhkoosr4b7
sts-backup-20210223-0300-ppss8nx40ykppss8nx40yk
===
job.batch "elasticsearch-list-snapshots-20210224t133115" deleted
----

The timestamp when the backup was taken is part of the backup name.

=== Delete Elasticsearch indices

[NOTE]
====
You can use the `--delete-all-indices` flag with the restore script to automatically delete all indices before restoring. For more information, see xref:/setup/data-management/backup_restore/kubernetes_backup.adoc#_restore_an_elasticsearch_snapshot[restore an Elasticsearch snapshot].
====

To manually delete the existing Elasticsearch indices and restore a snapshot, follow these steps:

. Stop indexing - scale down all deployments using Elasticsearch to 0:
+
[,bash]
----
kubectl scale --replicas=0 deployment -l observability.suse.com/scalable-during-es-restore="true"
----

. Open a port-forward to the Elasticsearch master:
+
[,bash]
----
kubectl port-forward service/suse-observability-elasticsearch-master 9200:9200
----

. Get a list of all indices:
+
[,bash]
----
curl "http://localhost:9200/_cat/indices?v=true"
----
+
The output should look like this:
+
[,bash]
----
health status index                              uuid                   pri rep docs.count docs.deleted store.size pri.store.size dataset.size
green  open   .ds-sts_k8s_logs-2025.09.28-004619 9p7RZwNCR-aQwInTMr5Bow   3   1   24511032            0        6gb            3gb          3gb
green  open   sts_topology_events-2025.10.01     86I2JZIeRzqWkK1dolHzhg   1   1    1576132            0    111.6mb         55.8mb       55.8mb
green  open   sts_topology_events-2025.10.02     T-bcrok_S1uVPLusQuCMxw   1   1     999748            0     75.2mb         37.6mb       37.6mb
green  open   .ds-sts_k8s_logs-2025.09.30-004653 rwlcAr0sTPe9NaImtJLIiw   3   1   24387607            0        6gb            3gb          3gb
green  open   sts_topology_events-2025.09.10     T0x-qvyUR2-dg4fyvdZIaQ   1   1    1746143            0    131.6mb         65.8mb       65.8mb
----

. Delete an index with a following command:
+
[,bash]
----
curl -X DELETE "http://localhost:9200/INDEX_NAME?pretty"
----
+
Replace `INDEX_NAME` with the name of the index to delete, for example:
+
[,bash]
----
curl -X DELETE "http://localhost:9200/sts_internal_events-2021.02.19?pretty"
----

. The output should be:
+
[,javascript]
----
{
"acknowledged" : true
}
----

=== Restore an Elasticsearch snapshot

[WARNING]
====
*When a snapshot is restored, existing indices won't be overwritten.*

You can use the `--delete-all-indices` flag to automatically delete all indices by the restoring script, or manually delete them as described in xref:/setup/data-management/backup_restore/kubernetes_backup.adoc#_delete_elasticsearch_indices[delete Elasticsearch indices].

====

[WARNING]
====
*If the Elasticsearch `PersistentVolumes` were re-created* (for example, by accidental deletion and following pod restart), you must recreate the Elasticsearch backup configuration by using either of the below two methods:

* Reinstalling the SUSE Observability Helm chart with the same configuration used for the initial installation (OR)
* Manually triggering the backup initialization CronJob:
+
[,bash]
----
kubectl create job --from=cronjob.batch/suse-observability-backup-init "suse-observability-backup-init-$(date +%s)"
----
====
To restore an Elasticsearch snapshot, select a snapshot name and pass it as the first parameter. You can optionally specify:

* A comma-separated list of indices to restore. If not specified, all indices matching the Helm value `backup.elasticsearch.scheduled.indices` will be restored. The default value is `"sts*"`).
* Introduced in version `2.6.1`, you can use the "--delete-all-indices" flag to automatically delete all existing indices before restoring.

==== Basic restore

[,bash]
----
./restore/restore-elasticsearch-snapshot.sh \
  sts-backup-20210223-0300-ppss8nx40ykppss8nx40yk
----

==== Restore specific indices

[,bash]
----
./restore/restore-elasticsearch-snapshot.sh \
  sts-backup-20210223-0300-ppss8nx40ykppss8nx40yk \
  "<INDEX_TO_RESTORE>,<INDEX_TO_RESTORE>"
----

==== Restore with automatic index deletion

[,bash]
----
./restore/restore-elasticsearch-snapshot.sh \
  sts-backup-20210223-0300-ppss8nx40ykppss8nx40yk \
  --delete-all-indices
----

When using the `--delete-all-indices` flag, confirm in the prompt to continue the action:

[,text]
----
WARNING: All indices will be deleted before restore!
Are you sure you want to continue? (yes/no): yes
=== Starting restore job
job.batch/elasticsearch-restore-20251003t115746 created
=== Restore job started. Follow the logs and clean up the job with the following commands:
kubectl logs --follow job/elasticsearch-restore-20251003t115746
kubectl delete job/elasticsearch-restore-20251003t115746
----

Follow the logs to see the deletion and restore progress:

[,text]
----
kubectl logs --follow job/elasticsearch-restore-20251003t115746

=== Deleting all indices matching pattern "sts*"...
Found indices to delete:
.ds-sts_k8s_logs-2025.10.03-000007
.ds-sts_k8s_logs-2025.10.03-000004
sts_topology_events-2025.10.02
sts_topology_events-2025.10.03
...
=== All indices deleted successfully
=== Restoring ElasticSearch snapshot "sts-backup-20251003-0925-aby7d1tgs9whvbm6qj04ug" (indices = "sts*") from snapshot repository "sts-backup" in bucket "sts-elasticsearch-backup"...
{
  "snapshot" : {
    "snapshot" : "sts-backup-20251003-0925-aby7d1tgs9whvbm6qj04ug",
    "indices" : [
      ".ds-sts_k8s_logs-2025.10.02-000003",
      "sts_topology_events-2025.10.02",
      "sts_topology_events-2025.10.03"
    ],
    "shards" : {
      "total" : 15,
      "failed" : 0,
      "successful" : 15
    }
  }
}
===
----

After the indices have been restored, scale up all deployments using Elasticsearch:

[,bash]
----
kubectl scale --replicas=1 deployment -l observability.suse.com/scalable-during-es-restore="true"
----
